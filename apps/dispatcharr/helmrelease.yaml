---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2beta2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app dispatcharr
  namespace: *app
spec:
  chart:
    spec:
      chart: app-template
      version: 4.6.2
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  interval: 5m
  releaseName: *app
  targetNamespace: *app
  values:
    defaultPodOptions:
      runtimeClassName: "nvidia-cdi"
    controllers:
      dispatcharr:
        type: statefulset
        containers:
          app:
            image:
              repository: ghcr.io/dispatcharr/dispatcharr
              tag: 0.20.0
            env:
              TZ: "Europe/Berlin"
              DISPATCHARR_ENV: aio
              POSTGRES_HOST: "localhost"
              REDIS_HOST: "localhost"
              CELERY_BROKER_URL: "redis://localhost:6379/0"
              DISPATCHARR_LOG_LEVEL: info
              NVIDIA_VISIBLE_DEVICES: "all"
              NVIDIA_DRIVER_CAPABILITIES: "all"
              CELERY_NICE_LEVEL: "5"      # Background tasks (higher = lower priority)
              FFMPEG_NICE_LEVEL: "0"      # Transcoding (keep responsive)
              STREAMLINK_NICE_LEVEL: "0"  # Stream fetching
            resources:
              requests:
                cpu: 1500m
              limits:
                nvidia.com/gpu: 1
            probes:
              liveness: &probes
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /
                    port: &port 9191
                  initialDelaySeconds: 30
                  periodSeconds: 60
                  timeoutSeconds: 30
                  failureThreshold: 5
              readiness: *probes
              startup:
                enabled: false
            securityContext:
              capabilities:
                add:
                  - SYS_NICE
              allowPrivilegeEscalation: false
          gluetun:
            dependsOn: app
            image:
              repository: ghcr.io/qdm12/gluetun
              tag: v3.41.1
            env:
              TZ: Europe/Berlin
              FIREWALL: on
              FIREWALL_INPUT_PORTS: 9191,9999
              VPN_TYPE: wireguard
              VPN_SERVICE_PROVIDER: mullvad
              SERVER_COUNTRIES: Germany
              UPDATER_PERIOD: 12h
              DOT: off
              DNS_ADDRESS: 192.168.178.91
              FIREWALL_OUTBOUND_SUBNETS: "192.168.178.91/32"
              HEALTH_SERVER_ADDRESS: "0.0.0.0:9999"
            envFrom:
              - secret: dispatcharr-vpn-config
            # Configure the container-specific securityContext
            securityContext:
              capabilities:
                add:
                  - NET_ADMIN
                  - SYS_MODULE
            lifecycle:
              postStart:
                exec:
                  command: [ "/bin/sh", "-c", "(ip rule del table 51820; ip -6 rule del table 51820) || true" ]
            probes:
              readiness:
                enabled: false
              liveness: &probes
                enabled: true
                custom: true
                spec:
                  exec:
                    command:
                      - /gluetun-entrypoint
                      - healthcheck
                  initialDelaySeconds: 30
                  periodSeconds: 10
                  timeoutSeconds: 5
                  failureThreshold: 5
              startup: *probes
    service:
      app:
        controller: *app
        ports:
          http:
            port: *port
      gluetun:
        controller: *app
        ports:
          http:
            port: 9999
    ingress:
      app:
        className: "traefik"
        annotations:
          kubernetes.io/tls-acme: "true"
          traefik.ingress.kubernetes.io/ssl-redirect: "true"
          cert-manager.io/cluster-issuer: "letsencrypt"
          traefik.ingress.kubernetes.io/enable-compression: "true"
          traefik.ingress.kubernetes.io/upstream-keepalive-connections: "32"
          traefik.ingress.kubernetes.io/upstream-keepalive-timeout: "60"
          traefik.ingress.kubernetes.io/proxy-connect-timeout: "10"
          traefik.ingress.kubernetes.io/proxy-buffering: "off"
          traefik.ingress.kubernetes.io/proxy-read-timeout: "3600"
          traefik.ingress.kubernetes.io/proxy-send-timeout: "3600"
          traefik.ingress.kubernetes.io/proxy-body-size: "0"
          traefik.ingress.kubernetes.io/proxy-buffer-size: "32k"
          traefik.ingress.kubernetes.io/websocket-services: "dispatcharr-app"
          traefik.ingress.kubernetes.io/router.middlewares: "kube-system-internal-only@kubernetescrd"
        hosts:
          - host: &host dispatcharr.internal.tmem.de
            paths:
              - path: /
                service:
                  identifier: app
                  port: http
        tls:
          - hosts:
              - *host
            secretName: dispatcharr-tls
    persistence:
      data:
        existingClaim: dispatcharr-config-pvc
        advancedMounts:
          dispatcharr:
            app:
              - path: /data
      logs:
        type: emptyDir
      tmp:
        type: emptyDir
      vpn-config:
        enabled: true
        type: secret
        name: dispatcharr-vpn-config
        advancedMounts:
          dispatcharr:
            gluetun:
              - path: /gluetun/wireguard/
